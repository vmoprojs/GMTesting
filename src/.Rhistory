max.order=c(5,5,1,1),
trace=FALSE,
cond.dists="sged",
with.forecast=TRUE,
forecast.length=1,
arma.sum=c(0,1e9),
cores=1,
ic="AIC",
garch.model="garch" )
{
require( fGarch )
require( parallel )
len = NROW( xx )
models = list( )
for( dist in cond.dists )
for( p in min.order[1]:max.order[1] )
for( q in min.order[2]:max.order[2] )
for( r in min.order[3]:max.order[3] )
for( s in min.order[4]:max.order[4] )
{
pq.sum = p + q
if( pq.sum <= arma.sum[2] && pq.sum >= arma.sum[1] )
{
models[[length( models ) + 1]] = list( order=c( p, q, r, s ), dist=dist )
}
}
res = mclapply( models,
garchAutoTryFit,
data=xx,
trace=trace,
ic=ic,
garch.model=garch.model,
forecast.length=forecast.length,
with.forecast=TRUE,
mc.cores=cores )
best.fit = NULL
best.ic = 1e9
for( rr in res )
{
if( !is.null( rr ) )
{
current.ic = rr@fit$ics[[ic]]
if( current.ic < best.ic )
{
best.ic = current.ic
best.fit = rr
}
}
}
if( best.ic < 1e9 )
{
return( best.fit )
}
return( NULL )
}
yg <- (datos.ts[,1]-mean(datos.ts[,1]))
Box.test(yg)
sol <- garchAuto(yg,max.order = c(2,2,2,2),trace = FALSE,cond.dists = "norm",cores = 8)
sol
sol@formula
sd(yg)
summary(sol@sigma.t)
View(datos)
View(dat)
setwd("~/Documents/Consultorias&Cursos/DataLectures")
dat <- rio::import("InflacionEC.csv")
datos <- dat[13:75,]
datos.ts <- ts(datos[,2:ncol(datos)],st = c(2016,1),fr = 12)
# ggplot2::autoplot(diff(datos.ts))
sumary(datos.ts[,1])
summary(datos.ts[,1])
2770-3000
2770-3000+1770
3000-1770
3000-1700
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Documents/Consultorias&Cursos/DataLectures")
dat <- rio::import("InflacionEC.csv")
datos <- dat[13:75,]
datos.ts <- ts(datos[,2:ncol(datos)],st = c(2016,1),fr = 12)
# ggplot2::autoplot(diff(datos.ts))
View(dat)
# http://www.quintuitive.com/2012/08/22/arma-models-for-trading/
# https://www.r-bloggers.com/2013/03/automatic-armagarch-selection-in-parallel/
# https://gist.github.com/ivannp/5198580
garchAutoTryFit = function(
ll,
data,
trace=FALSE,
forecast.length=1,
with.forecast=TRUE,
ic="AIC",
garch.model="garch" )
{
xxx = paste( sep="",
"~ arma(", ll$order[1], ",", ll$order[2], ")+",
garch.model,
"(", ll$order[3], ",", ll$order[4], ")" )
formula = formula(paste0(xxx,collapse = "")  )
fit = tryCatch( suppressWarnings(garchFit( formula=formula,
data=data,
trace=FALSE,
cond.dist=ll$dist )),
error=function( err ) TRUE
# ,warning=function( warn ) FALSE
)
# print(fit)
pp = NULL
if( !is.logical( fit ) ) {
if( with.forecast ) {
pp = tryCatch( predict( fit,
n.ahead=forecast.length,
doplot=FALSE ),
error=function( err ) FALSE,
warning=function( warn ) FALSE )
if( is.logical( pp ) ) {
fit = NULL
}
}
} else {
fit = NULL
}
if( trace ) {
if( is.null( fit ) ) {
cat( paste( sep="",
"   Analyzing (", ll$order[1], ",", ll$order[2],
",", ll$order[3], ",", ll$order[4], ") with ",
ll$dist, " distribution done.",
"Bad model.\n" ) )
} else {
if( with.forecast ) {
cat( paste( sep="",
"   Analyzing (", ll$order[1], ",", ll$order[2], ",",
ll$order[3], ",", ll$order[4], ") with ",
ll$dist, " distribution done.",
"Good model. ", ic, " = ", round(fit@fit$ics[[ic]],6),
", forecast: ",
paste( collapse=",", round(pp[,1],4) ), "\n" ) )
} else {
cat( paste( sep="",
"   Analyzing (", ll[1], ",", ll[2], ",", ll[3], ",", ll[4], ") with ",
ll$dist, " distribution done.",
"Good model. ", ic, " = ", round(fit@fit$ics[[ic]],6), "\n" ) )
}
}
}
return( fit )
}
garchAuto = function(
xx,
min.order=c(0,0,1,1),
max.order=c(5,5,1,1),
trace=FALSE,
cond.dists="sged",
with.forecast=TRUE,
forecast.length=1,
arma.sum=c(0,1e9),
cores=1,
ic="AIC",
garch.model="garch" )
{
require( fGarch )
require( parallel )
len = NROW( xx )
models = list( )
for( dist in cond.dists )
for( p in min.order[1]:max.order[1] )
for( q in min.order[2]:max.order[2] )
for( r in min.order[3]:max.order[3] )
for( s in min.order[4]:max.order[4] )
{
pq.sum = p + q
if( pq.sum <= arma.sum[2] && pq.sum >= arma.sum[1] )
{
models[[length( models ) + 1]] = list( order=c( p, q, r, s ), dist=dist )
}
}
res = mclapply( models,
garchAutoTryFit,
data=xx,
trace=trace,
ic=ic,
garch.model=garch.model,
forecast.length=forecast.length,
with.forecast=TRUE,
mc.cores=cores )
best.fit = NULL
best.ic = 1e9
for( rr in res )
{
if( !is.null( rr ) )
{
current.ic = rr@fit$ics[[ic]]
if( current.ic < best.ic )
{
best.ic = current.ic
best.fit = rr
}
}
}
if( best.ic < 1e9 )
{
return( best.fit )
}
return( NULL )
}
yg <- (datos.ts[,1]-mean(datos.ts[,1]))
Box.test(yg)
library(tseries)
adf.test(yg)
yg <- (diff(datos.ts[,1]))
Box.test(yg)
mean(yg)
mean(yg)
Box.test(yg)
adf.test(yg)
yg <- (diff(datos.ts[,1]))
mean(yg)
Box.test(yg)
adf.test(yg)
sol <- garchAuto(yg,max.order = c(2,2,2,2),trace = FALSE,cond.dists = "norm",cores = 8)
sol
sol
sol@formula
sd(yg)
summary(sol@sigma.t)
datosCrec <- diff(log(datos.ts))
library(tseries)
sol <- NULL
for(i in 1:ncol(datosCrec))
{
# i = 1
aux <- pp.test(datosCrec[,i]) #Ho: non stationarity
sol <- c(sol,aux$p.value)
}
names(sol) <- colnames(datosCrec)
sol
datos.ts
# Escogemos el mejor modelo
library(vars)
VARselect(datosCrec, type = "none", lag.max = 5)
modVar <- VAR(datosCrec, 2, type = "none")
summary(modVar)
modVar <- VAR(datosCrec, 1, type = "none")
summary(modVar)
# Revisamos los residuos del modelo:
rr <- resid(modVar)
Box.test(rr[,1]);Box.test(rr[,2]);Box.test(rr[,3]);Box.test(rr[,4]);Box.test(rr[,5])
Box.test(rr[,1],order = 12)
?Box.test
Box.test(rr[,1],lag = 12);
Box.test(rr[,1],lag = 12);Box.test(rr[,2],lag = 12);Box.test(rr[,3],lag = 12);Box.test(rr[,4],lag = 12);Box.test(rr[,5],lag = 12)
modVar <- VAR(datosCrec, 2, type = "none")
summary(modVar)
# Revisamos los residuos del modelo:
rr <- resid(modVar)
Box.test(rr[,1],lag = 12);Box.test(rr[,2],lag = 12);Box.test(rr[,3],lag = 12);Box.test(rr[,4],lag = 12);Box.test(rr[,5],lag = 12)
variables <- colnames(datosCrec)
i = 3
j = 1
ir.1 <- irf(modVar,  response = variables[j], n.ahead = 12, ortho = FALSE)
par(mfrow = c(2,2))
plot(ir.1)
par(mfrow = c(1,1))
variables <- colnames(datosCrec)
i = 3
j = 1
ir.1 <- irf(modVar,  response = variables[j], n.ahead = 12, ortho = TRUE)
par(mfrow = c(2,2))
plot(ir.1)
par(mfrow = c(1,1))
variables[j]
variables[j]
variables
variables <- colnames(datosCrec)
i = 3
j = 1
ir.1 <- irf(modVar,  impulse = variables[j], n.ahead = 12, ortho = TRUE)
ir.1 <- irf(modVar,  impulse = variables[j], n.ahead = 12, ortho = TRUE)
par(mfrow = c(2,2))
plot(ir.1)
par(mfrow = c(1,1))
plot(ir.1)
par(mfrow = c(1,1))
plot(ir.1)
variables <- colnames(datosCrec)
i = 3
j = 1
ir.2 <- irf(modVar,  response = variables[j], n.ahead = 12, ortho = TRUE,cumulative = TRUE)
plot(ir.2)
i = 1
for(i in 1:ncol(datosCrec))
{
variable = colnames(datosCrec)[i]
cat("\n-------",variable,"--------\n")
print(causality(modVar,cause = variable))
}
y <- datosCrec[,"IPC"]
x <- datosCrec[,"RI"]
m1 <- lm(y~x)
summary(m1)
po.test(cbind(y,x)) #Ho: no cointegradas
resm1 <- resid(m1)
pp.test(resm1)
y <- datosCrec[,"IPC"]
x <- datosCrec[,"RI"]
m1 <- lm(y~x)
po.test(cbind(y,x)) #Ho: no cointegradas
resm1 <- resid(m1)
pp.test(resm1)
Box.test(resm1,type = "Ljung-Box",lag = 5) #Ho: independencia
n <- length(y)
ecm1 <- lm(diff(y)~diff(x)+lag(resm1[2:(n)],1))
summary(ecm1)
library(ecm)
mm <- ecm(y,xeq = x,xtr = x)
mm <- ecm(y,xeq = data.frame(x),xtr = data.frame(x))
data.frame(x)
xeq <- xtr <- data.frame(x = x)
xeq <- xtr <- data.frame(x = x)
xeq
mm <- ecm(y,xeq = xeq$x,xtr = xtr$x)
n <- length(y)
ecm1 <- lm(diff(y)~diff(x)+lag(resm1[2:(n)],1))
summary(ecm1)
trn <- data.frame(y = as.numeric(y),x = as.numeric(x))
xeq <- xtr <- data.frame(trn$x)
(ecm2 <- ecm(trn$y, xeq,xtr ))
summary(ecm2)
8.429e-01/1.379e-03
8.429e-01/1.662e-04
1.662e-04/8.429e-01
summary(ecm1)
summary(ecm1)
pp.test(y)
pp.test(x)
y <- datos[,"IPC"]
x <- datos[,"RI"]
pp.test(y)
pp.test(x)
x <- datos[,"IR"]
pp.test(y)
pp.test(x)
x <- datos[,"RI"]
pp.test(y)
pp.test(x)
?pp.test
y <- datos[,"IPC"]
x <- datos[,"RI"]
pp.test(y)
pp.test(x)
pp.test(y)
pp.test(x)
m1 <- lm(y~x)
po.test(cbind(y,x)) #Ho: no cointegradas
resm1 <- resid(m1)
pp.test(resm1)
Box.test(resm1,type = "Ljung-Box",lag = 5) #Ho: independencia
n <- length(y)
ecm1 <- lm(diff(y)~diff(x)+lag(resm1[2:(n)],1))
summary(ecm1)
pp.test(y)
pp.test(x)
adf.test(x,lag=12)
adf.test(x,lags=12)
?adf.test
adf.test(x,k=12)
curve(1/(1+exp(-x)),xlim = c(-10,10))
x1 <- c(37,
24,
32,
22,
24)
x2 <- c(185,
180,
173,
170,
178)
y <- c(1,
1,
1,
0,
0)
glm(y~x1+x2,family = binomial(link = "logit"))
library(rhub)
library(rhub)
platforms()
list_validated_emails()
validate_email(email = "victor.morales@uv.cl", token = "2e021e8d59834947b8c7c2ac9966d22a")
rhub::check("GeoModels",platform = "macos-highsierra-release",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels",platform = "macos-highsierra-release-cran",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels",platform = "macos-m1-bigsur-release",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels",platform = "solaris-x86-patched",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels",platform = "solaris-x86-patched-ods",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels",platform = "macos-highsierra-release",
valgrind = TRUE,email = "victor.morales@uv.cl")
rhub::check("GeoModels_1.0.1.tar.gz",platform = "macos-highsierra-release",
valgrind = TRUE,email = "victor.morales@uv.cl")
version
uu <- "https://github.com/vmoprojs/DataLectures/blob/master/Amazon_Unlocked_Mobile.zip"
#creamos un par de archivos temporales
temp <- tempfile()
temp2 <- tempfile()
download.file(uu,temp)
#descomprimir en 'temp' y guardarlo en 'temp2'
unzip(zipfile = temp, exdir = temp2)
#encontramos los archivos SHP
#el $ al final de ".shp$" asegura que no encontremos archivos del tipo .shp.xml
your_csv_file <- list.files(temp2, pattern = ".csv$",full.names=TRUE)
your_csv_file
temp <- tempfile()
temp2 <- tempfile()
#decargamos el zip folder y lo guardamos en 'temp'
download.file(uu,temp)
temp
temp2
unzip(zipfile = temp, exdir = temp2)
?unzip
#encontramos los archivos SHP
#el $ al final de ".shp$" asegura que no encontremos archivos del tipo .shp.xml
your_csv_file <- list.files(temp2, pattern = ".csv$",full.names=TRUE)
your_csv_file
temp2
?list.files
list.files(temp2, pattern = ".csv$")
?unzip
read_git_shp <- function(uu)
{
#creamos un par de archivos temporales
temp <- tempfile()
temp2 <- tempfile()
#decargamos el zip folder y lo guardamos en 'temp'
download.file(uu,temp)
#descomprimir en 'temp' y guardarlo en 'temp2'
unzip(zipfile = temp, exdir = temp2)
#encontramos los archivos SHP
#el $ al final de ".shp$" asegura que no encontremos archivos del tipo .shp.xml
your_csv_file <- list.files(temp2, pattern = ".csv$",full.names=TRUE)
ff = strsplit(your_csv_file,"/")
ff = unlist(ff)
ff = ff[length(ff)]
ff = strsplit(ff,".csv")
ff = unlist(ff)
datos = rgdal::readOGR(your_csv_file,layer = ff)
unlink(temp)
unlink(temp2)
return(datos)
}
read_git_shp(uu)
uu <- "https://github.com/vmoprojs/DataLectures/raw/master/Amazon_Unlocked_Mobile.zip"
read_git_shp(uu)
uu <- "https://github.com/vmoprojs/DataLectures/raw/master/Amazon_Unlocked_Mobile.zip"
#creamos un par de archivos temporales
temp <- tempfile()
temp2 <- tempfile()
download.file(uu,temp)
#descomprimir en 'temp' y guardarlo en 'temp2'
unzip(zipfile = temp, exdir = temp2)
#encontramos los archivos SHP
#el $ al final de ".shp$" asegura que no encontremos archivos del tipo .shp.xml
your_csv_file <- list.files(temp2, pattern = ".csv$",full.names=TRUE)
ff = strsplit(your_csv_file,"/")
ff = unlist(ff)
ff = ff[length(ff)]
ff = strsplit(ff,".csv")
ff = unlist(ff)
datos = rio::import(your_csv_file,layer = ff)
unlink(temp)
unlink(temp2)
head(datos)
library(GeoModels)
?GeoCovariogram
# Simulation of a bivariate spatial Gaussian random field:
set.seed(892)
# Define the spatial-coordinates of the points:
x = runif(500, -1, 1)
y = runif(500, -1, 1)
coords=cbind(x,y)
# Simulation of a bivariate Gaussian Random field
# with matern (cross)  covariance function
scale_1 = 0.18/3
scale_2 = 0.2/3
scale_12 = 0.15/3
sill_1=1
sill_2=1
smooth=0.5
pcol=0.2
param=list(mean_1=0,mean_2=0,scale_1=scale_1,scale_2=scale_2,
sill_1=sill_1,sill_2=sill_2,nugget_1=0,nugget_2=0,
smooth_1=smooth,smooth_2=smooth,pcol=pcol)
data = GeoSim(coordx=coords, corrmodel="Bi_Matern_contr", param=param)$data
# Empirical bivariate variogram estimation:
biv_vario=GeoVariogram(data,coordx=coords, bivariate=TRUE,maxdist=c(0.5,0.5,0.5))
# selecting fixed and estimating parameters
fixed=list(mean_1=0,mean_2=0,nugget_1=0,nugget_2=0,
smooth_1=smooth,smooth_2=smooth)
start=list(sill_1=var(data[1,]),sill_2=var(data[2,]),
scale_1=scale_1,scale_2=scale_2,
pcol=cor(data[1,],data[2,]))
# Maximum likelihood fitting of the bivariate random field:
fit= GeoFit(data, coordx=coords, corrmodel="Bi_Matern_contr",likelihood="Marginal",
optimizer="BFGS", type="Pairwise",
start=start,fixed=fixed,neighb=c(2,2,2))
rm( list =ls())
setwd("~/Documents/Software/GeoModels/GeoStage/Stage/CRAN/New/")
setwd("~/Documents/Software/GeoModels/GeoStage/Stage/CRAN/New/GeoModels/src")
dyn.load("GeoModels.so")
